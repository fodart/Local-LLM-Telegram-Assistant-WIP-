# Local-LLM-Telegram-Assistant-WIP-
 A work-in-progress local AI assistant built around a self-hosted LLM, a Flask core service, and a Telegram bot interface.

 The project is focused on clean architecture, separation of concerns, and stability, rather than quick hacks.
 It is designed to be extended step by step into a more capable AI agent.

# âœ¨ Key Ideas

 **ğŸ§© Decoupled architecture**

* Telegram bot acts only as a transport layer
 * Flask acts as the core logic / brain
 * LLM is accessed only through the core service

 **ğŸ§  Local-first**
 
 * Uses a locally running LLM (via Ollama)
* No external AI APIs required

**ğŸ” Conversation memory**

* Per-chat session memory
* Context is explicitly managed and passed to the model

**âš™ï¸ Async-safe Telegram handling**

* Long-running LLM requests do not block Telegram polling
* Background tasks are used for processing and replies

# ğŸ—ï¸ Current Architecture
```
Telegram User
     â†“
Telegram Bot (async, lightweight)
     â†“ HTTP POST
Flask Core Service
     â†“
Local LLM (Ollama)
     â†“
Flask JSON response
     â†“
Telegram Bot sends reply
```

# ğŸ“¦ Components

* Telegram Bot

  - Receives messages

  - Sends them to the core service

  - Delivers responses back to the user

* Flask Core

  - Handles message routing

  - Manages conversation sessions

  - Communicates with the local LLM

* LLM

  - Runs locally via Ollama

  - Can be swapped or extended
 
# ğŸš§ Project Status

**This project is under active development.**

**Implemented:**

* Basic Telegram â†” Flask communication
* Local LLM integration
* Per-chat memory (in-memory)
* Async-safe message processing

**Planned / in progress:**
* Better memory management (limits, reset, persistence)
* Action / tool execution layer
* Multiple interaction modes (chat vs actions)
* Improved error handling and logging
* Optional web or CLI interface

# âš ï¸ Notes

* This is not a production-ready system yet.
* In-memory sessions will reset on restart.
* APIs and internal structure may change.

# ğŸ¯ Goals

The long-term goal is to evolve this project into a flexible local AI agent core that can be connected to different interfaces (Telegram, web, CLI) while keeping the core logic isolated and testable.

# ğŸ“„ License

**MIT**
